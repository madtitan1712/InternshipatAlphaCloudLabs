# Internship at AlphaCloudLabs
This github repo contains my work done in a 10 day internship at Alphacloud labs, Chennai.

# Day 1:
  
  Started today. 

  - Refreshed Python
  - scraping
  - An small Scraper that scrapes text from an targeted Website
  - Used BS4 and the requests Library in python to create a program that can scrape websites

# Day 2:

  Learnt:
  - Beautiful soup
  - Requests library
  - Selenium
  - lxml
  - Scrapy
  Created a new python script which combed through any wikipedia page and collected resources from it in the form of text and Images

# Day 3:

  Selenium Webdriver:
  
  An API, that can interact and control browsers.

  Selenium can manipulate both browsers and automate them. This makes them an ideal choice for testing websites. 
  All elements can be found or be manipulated using their attributes like
    - Xpath
    - ID
    - Name
    - Class_name
    - CSS ID
  
  Xpath is something that is used to locate objects on the DOM. Here everything is like an tree based structure, so you can call the children and parent objects easily. 
  
  Reference for Xpath: [W3Schools.com](https://www.w3schools.com/xml/xpath_syntax.asp)

# Day 4:
  
Wrote 4 scripts to:
  - Extract the F1 driver stadings from the Website
  - Extract the F1 Race Results from a previous season
  - Scrape the top 250 movies list from IMDB
  - Scrape Instagram post images.

Learning objectives:
  - CSV module
  - Image Scraping basics

Learnt to scrape the main post images from Instagram. If the image is a carousel, then the script needs how many images are there in the carousel in order to scrape it

# Day 5:

Numpy Basics:
  - How to initialize a numpy array
  - basic functions on numpy
  - mathematical functions on numpy
  - append/sort/insert/delete
Image Scraping:
  - Scraped a F1 related Website
  - Scraped a Premier League website to scour images
   
# Day 6:

Writing a script to scrape data from google on sports schedules.

This program uses selenium to target webelements and extract information from them.
  
Numpy Basics:
  - Open Google and search for the given sport nfl 
  - then navigate to the table with the schedule
  - then click on it and scrape the team names and their dates

# Day 7:

The script of scraping the data from google on sports schedules was improved.

Added Features:
  - Can open every single match data and collect the venue of the match
  - Used Selenium to automate the feature
  - Automated by clicking on each element and then opens the next element

# Day 8:
**DataFrame:**

- Dataframe is something like a excel spreadsheet or a SQL database. Here the data are arranged in a 2 dimensional structure so that it can be manipulated easily.
- One of the most popular dataframe related library is Pandas in Python
- Dataframes are used in cleaning, arranging, and interpreting the data in a much easier way.
- Pandas is one of the way to achieve this

**Data Cleaning:**

- Process of correcting, removing errors and inconsistencies in the dataset

Some of them are:

- Missing data → filling in the data with their Mean or Mode
- Outliers → Identifying and removing of the outliers which lie outside of the dataset’s boundaries by using functions
- Normalization → setting the data according to certain formats can make the interpretation of the data easier and accurate
- Duplicates → removing of the redundant data can help in the processing speeds of the data

This can help in improving:

- Accuracy
- Reliability
- Efficiency

**Data Processing:**

Data processing is converting raw data into meaningful information using certain operations.

Their stages are:

- Data collection
- Data preparation
- Data input
- Processing
- Data output/interpretation

**EDA:**

EDA Stands for **E**xploratory **D**ata **A**nalysis. It is used to summarize the given dataset in a visual way. When data is presented visually, more information can be derived, leading to a more detailed view on the data. This is done using EDA. EDA is done using a few libraries and also a few apps. Some of them are

- Python libraries such as Matplotlib
- Tableau
- Microsoft PowerBi Analytics

# Day 9:

Have Learnt more on the Pandas library. Solved 15 leetcode problems on "introduction to pandas". Completed the Leetcode Learning plan for the pandas module and earned a badge.

Also took a dataset from the statsmodels library and performed a few operations on the dataset using pandas. Performed a set of operations like, 
- finding certain information on the dataset
- applying a custom function over the entire dataframe
